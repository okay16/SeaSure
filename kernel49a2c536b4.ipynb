{"metadata":{"kernelspec":{"name":"python3","language":"python","display_name":"Python 3"},"language_info":{"mimetype":"text/x-python","nbconvert_exporter":"python","codemirror_mode":{"name":"ipython","version":3},"name":"python","version":"3.6.1","pygments_lexer":"ipython3","file_extension":".py"}},"nbformat":4,"cells":[{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_execution_state":"idle","_uuid":"b690ba5fc07e34ca609aee193503f693148be190","_cell_guid":"180c576c-7af1-4e6b-b40d-e2c03b81758f","collapsed":true},"source":"import numpy as np\nimport tensorflow as tf\nimport pandas as pd\nfrom sklearn.preprocessing import LabelEncoder"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_execution_state":"idle","_uuid":"b59dd01dfea86ab08cd5e6e9f3e642ef51c93ed4","_cell_guid":"bfb69ad5-d7a1-4d67-ad23-3627a7e83fa7","collapsed":true},"source":"import collections\nimport random\nimport time\n\nclass WordModel:\n    \n    def __init__(self, batch_size, dimension_size, learning_rate, vocabulary_size):\n        \n        self.train_inputs = tf.placeholder(tf.int32, shape = [batch_size])\n        self.train_labels = tf.placeholder(tf.int32, shape = [batch_size, 1])\n        \n        # randomly generated initial value for each word dimension, between -1.0 to 1.0\n        embeddings = tf.Variable(tf.random_uniform([vocabulary_size, dimension_size], -1.0, 1.0))\n        \n        # find train_inputs from embeddings\n        embed = tf.nn.embedding_lookup(embeddings, self.train_inputs)\n        \n        # estimation for not normalized dataset\n        self.nce_weights = tf.Variable(tf.truncated_normal([vocabulary_size, dimension_size], stddev = 1.0 / np.sqrt(dimension_size)))\n        \n        # each node have their own bias\n        self.nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n        \n        # calculate loss from nce, then calculate mean\n        self.loss = tf.reduce_mean(tf.nn.nce_loss(weights = self.nce_weights, biases = self.nce_biases, labels = self.train_labels,\n                                                  inputs = embed, num_sampled = batch_size / 2, num_classes = vocabulary_size))\n        \n        self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)\n        \n        # normalize the data by simply reduce sum\n        self.norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n        \n        # normalizing each embed\n        self.normalized_embeddings = embeddings / self.norm\n\ndef read_data(filename):\n\n    dataset = pd.read_csv(filename)\n    rows = dataset.shape[0]\n    print('there are', rows, 'total rows')\n    \n    # last column is our target\n    label = dataset.ix[:, -1:].values\n        \n    # get second and third column values\n    concated = []\n    data = dataset.ix[:, 1:3].values\n    \n    for i in range(data.shape[0]):\n        string = \"\"\n            \n        for k in range(data.shape[1]):\n            string += data[i][k] + \" \"\n            \n        concated.append(string) \n    \n    # get all split strings from second column and second last column\n    dataset = dataset.ix[:, 1:3].values\n    string = []\n    for i in range(dataset.shape[0]):\n        for k in range(dataset.shape[1]):\n            string += dataset[i][k].split()\n    \n    return string, concated, label, list(set(string))\n\ndef build_dataset(words, vocabulary_size):\n    count = []\n    # extend count\n    # sorted decending order of words\n    count.extend(collections.Counter(words).most_common(vocabulary_size))\n\n    dictionary = dict()\n    for word, _ in count:\n        #simply add dictionary of word, used frequently placed top\n        dictionary[word] = len(dictionary)\n\n    data = []\n    unk_count = 0\n    for word in words:\n        if word in dictionary:\n            index = dictionary[word]\n\n        data.append(index)\n    \n    reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n    return data, dictionary, reverse_dictionary\n\ndef generate_batch_skipgram(words, batch_size, num_skips, skip_window):\n    data_index = 0\n    \n    #check batch_size able to convert into number of skip in skip-grams method\n    assert batch_size % num_skips == 0\n    \n    assert num_skips <= 2 * skip_window\n    \n    # create batch for model input\n    batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n    labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n    span = 2 * skip_window + 1\n    \n    # a buffer to placed skip-grams sentence\n    buffer = collections.deque(maxlen=span)\n    \n    for i in range(span):\n        buffer.append(words[data_index])\n        data_index = (data_index + 1) % len(words)\n    \n    for i in range(batch_size // num_skips):\n        target = skip_window\n        targets_to_avoid = [skip_window]\n        \n        for j in range(num_skips):\n            \n            while target in targets_to_avoid:\n                # random a word from the sentence\n                # if random word still a word already chosen, simply keep looping\n                target = random.randint(0, span - 1)\n            \n            targets_to_avoid.append(target)\n            batch[i * num_skips + j] = buffer[skip_window]\n            labels[i * num_skips + j, 0] = buffer[target]\n        \n        buffer.append(words[data_index])\n        data_index = (data_index + 1) % len(words)\n    \n    data_index = (data_index + len(words) - span) % len(words)\n    return batch, labels\n\ndef generatevector(dimension, batch_size, skip_size, skip_window, num_skips, iteration, words_real):\n    \n    print('data size: ', len(words_real))\n    data, dictionary, reverse_dictionary = build_dataset(words_real, len(words_real))\n    \n    sess = tf.InteractiveSession()\n    print('Creating Word2Vec model.')\n    \n    model = WordModel(batch_size, dimension, 0.01, len(dictionary))\n    sess.run(tf.global_variables_initializer())\n    \n    last_time = time.time()\n    \n    for step in range(iteration):\n        new_time = time.time()\n        batch_inputs, batch_labels = generate_batch_skipgram(data, batch_size, num_skips, skip_window)\n        feed_dict = {model.train_inputs: batch_inputs, model.train_labels: batch_labels}\n        \n        _, loss = sess.run([model.optimizer, model.loss], feed_dict=feed_dict)\n        \n        if ((step + 1) % 1000) == 0:\n            print('epoch: ', step + 1, ', loss: ', loss, ', speed: ', time.time() - new_time)\n    \n    tf.reset_default_graph()       \n    return dictionary, reverse_dictionary, model.normalized_embeddings.eval()"},{"cell_type":"markdown","metadata":{"_uuid":"77fe22365877d038ed2e3d88cdec0855a95a98e5","_cell_guid":"7f41d217-12e7-4f3f-8a93-7738381124e2"},"source":"I will define my global variables below"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_execution_state":"idle","_uuid":"b23e21f703284677faad3943db534d098378b8bf","_cell_guid":"f9dff49e-86e2-4905-9b81-2f69a31d66e6","collapsed":true},"source":"dimension = 32\nskip_size = 8\nskip_window = 1\nnum_skips = 2\niteration_train_vectors = 5000\n\nnum_layers = 2\nsize_layer = 256\nlearning_rate = 0.001\nepoch = 10\nbatch = 30"},{"cell_type":"markdown","metadata":{"_uuid":"ed6853adc0622295904ed9303498fc22ca7c6ae4","_cell_guid":"2a462118-368f-4573-a6ec-173e14916f3b"},"source":"Below is my training dataset, i assume testing dataset vocabulary is a subset of training vocabulary"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_execution_state":"idle","_uuid":"9937e0bee342a39f026eed4087f6fcf923f051c4","_cell_guid":"1188f6d8-21fa-4b80-a4bb-bd3def2a2562"},"source":"string, data, label, vocab = read_data('../input/train.csv')\nlabel_encode = LabelEncoder().fit_transform(label)\ndictionary, reverse_dictionary, vectors = generatevector(dimension, dimension, skip_size, skip_window, num_skips, iteration_train_vectors, string)"},{"cell_type":"markdown","metadata":{"_uuid":"d7197c5b22e128f34924b19e04fb54dcea549c98","_cell_guid":"941b3f1d-7d55-46cf-8e9f-43f5827a51cc"},"source":"Lets visualize our word vector in 2d space"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_execution_state":"idle","_uuid":"d4ae4dec0316acc7fbafbd2548c979733789dd77","_cell_guid":"1ad6b84b-e605-453e-9a8b-99c32f6827d4","collapsed":true},"source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.manifold import TSNE\nsns.set()\n\nembed = TSNE(n_components = 2).fit_transform(vectors)\nplt.figure(figsize = (32, 32))\n\nfor i, label in enumerate(reverse_dictionary):\n    x, y = embed[i, :]\n    plt.scatter(x, y, c = 'g')\n    plt.annotate(label, xy = (x, y), xytext = (5, 2), textcoords = 'offset points', ha = 'right', va = 'bottom')\n\nplt.show()"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_uuid":"14140726bc004b95f0b492261840c0a42dfaeb6a","_cell_guid":"e974cea1-bda0-45d2-a310-67e7e31c0617","collapsed":true},"source":"bagofword = np.zeros((len(data), len(vocab)))\nfor i in range(len(data)):\n    for _, text in enumerate(data[i].split()):\n        bagofword[i, vocab.index(text)] += 1.0\n        \nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import Normalizer\n\nbagofword = Normalizer().fit_transform(bagofword)\ndata_visual = PCA(n_components = 2).fit_transform(bagofword)\npalette = ['r', 'b']\ndata_label = ['negative', 'positive']\n\nplt.figure(figsize = (18, 18))\nfor no, _ in enumerate(np.unique(label_encode)):\n    plt.scatter(data_visual[label_encode == no, 0], data_visual[label_encode == no, 1], c = palette[no], label = data_label[no], alpha = 0.5)\n    \nplt.legend()\nplt.show()"},{"cell_type":"markdown","metadata":{"_uuid":"f5265e304e74b77a1c13a82b6d41927c67f4e3ad","_cell_guid":"a66a8478-6c40-4b3b-b990-038757ebb658"},"source":"WIth this 2 components, actually you can use any machine learning classifier, the data not very much overlapped each other."},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_execution_state":"idle","_uuid":"d4394957536c26e5654229d35fadfb673604cbd3","_cell_guid":"a8232a00-5ae7-4abe-b202-4f1c6dc84bb2","collapsed":true},"source":"dimension_input = len(vocab)\nprint('dimension size: ', str(dimension_input))\nprint('sentence size: ', len(data))"},{"cell_type":"markdown","metadata":{"_uuid":"44afe108bbac1e07da6cf0e61cdb6c3fec340522","_cell_guid":"e78e96af-c3e3-4ef6-834f-bc3aa4b10bc8"},"source":"Now we create our Recurrent Neural Network"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_execution_state":"idle","_uuid":"3798861efbb004ac2689d69ff58106eed7b6f863","_cell_guid":"fec9704a-dd50-4787-9ce4-47bf58e4857f","collapsed":true},"source":"class Model:\n    \n    def __init__(self, num_layers, size_layer, dimension_input, dimension_output, learning_rate):\n        \n        def lstm_cell():\n            return tf.nn.rnn_cell.LSTMCell(size_layer, activation = tf.nn.relu)\n        \n        self.rnn_cells = tf.nn.rnn_cell.MultiRNNCell([lstm_cell() for _ in range(num_layers)])\n        \n        # [dimension of word, batch size, dimension input]\n        self.X = tf.placeholder(tf.float32, [None, None, dimension_input])\n        \n        #[batch size, dimension input]\n        self.Y = tf.placeholder(tf.float32, [None, dimension_output])\n        \n        self.outputs, self.last_state = tf.nn.dynamic_rnn(self.rnn_cells, self.X, dtype = tf.float32)\n        \n        self.rnn_W = tf.Variable(tf.random_normal((size_layer, dimension_output)))\n        self.rnn_B = tf.Variable(tf.random_normal([dimension_output]))\n        \n        self.logits = tf.matmul(self.outputs[-1], self.rnn_W) + self.rnn_B\n        \n        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = self.logits, labels = self.Y))\n        \n        self.optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(self.cost)\n        \n        self.correct_pred = tf.equal(tf.argmax(self.logits, 1), tf.argmax(self.Y, 1))\n        \n        self.accuracy = tf.reduce_mean(tf.cast(self.correct_pred, tf.float32))"},{"cell_type":"markdown","metadata":{"_uuid":"921bb7769ff06c943175b56eb64fb9462a0c213a","_cell_guid":"71bd94dc-442e-4385-84c9-d855a0ca87c7"},"source":"Start our session! but wait, i forgot to split the dataset for validation"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_execution_state":"idle","_uuid":"2c16163e8a8e2913417ea9aca468d72eaf18d188","_cell_guid":"4726803b-8e88-4477-856e-b2794bde39ac","collapsed":true},"source":"from sklearn.cross_validation import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(data, label_encode, test_size = 0.15)"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_execution_state":"idle","_uuid":"80c28a01a84f7a0e00c60567e9dccca9cba5361c","_cell_guid":"2bbe3b79-645d-44e1-aae3-0545a8c6f561","collapsed":true},"source":"tf.reset_default_graph()\nsess = tf.InteractiveSession()\nmodel = Model(num_layers, size_layer, dimension_input, np.unique(label_encode).shape[0], learning_rate)\nsess.run(tf.global_variables_initializer())\nsaver = tf.train.Saver(tf.global_variables())"},{"outputs":[],"cell_type":"code","execution_count":null,"metadata":{"_execution_state":"idle","_uuid":"95826e0c814a14c789899a77dd9481209fb8b250","_cell_guid":"f5b87a4a-8e43-4af3-95aa-5ab6f882428e","collapsed":true},"source":"from sklearn import metrics\n\nACC_TRAIN, ACC_TEST, LOST = [], [], []\nfor i in range(epoch):\n    total_cost = 0\n    total_accuracy = 0\n    last_time = time.time()\n    \n    for n in range(0, (len(X_train) // batch) * batch, batch):\n        batch_x = np.zeros((dimension, batch, dimension_input))\n        batch_y = np.zeros((batch, np.unique(Y_train).shape[0]))\n        for k in range(batch):\n            emb_data = np.zeros((dimension, dimension_input), dtype = np.float32)\n            for _, text in enumerate(X_train[n + k].split()):\n                # if the word got in the vocab\n                try:\n                    emb_data[:, vocab.index(text)] += vectors[dictionary[text], :]\n                # if not, skip\n                except:\n                    continue\n\n            batch_y[k, int(Y_train[n + k])] = 1.0\n            batch_x[:, k, :] = emb_data[:, :]\n            \n        loss, _ = sess.run([model.cost, model.optimizer], feed_dict = {model.X : batch_x, model.Y : batch_y})\n        total_accuracy += sess.run(model.accuracy, feed_dict = {model.X : batch_x, model.Y : batch_y})\n        total_cost += loss\n        \n    total_cost /= (len(X_train) // batch)\n    total_accuracy /= (len(X_train) // batch)\n    times = (time.time() - last_time) / (len(X_train) // batch)\n        \n    ACC_TRAIN.append(total_accuracy)\n    LOST.append(total_cost)\n        \n    print('epoch: ', i + 1, ', loss: ', total_cost, ', accuracy train: ', total_accuracy, 's / batch: ', times)\n        \n    batch_x = np.zeros((dimension, Y_test.shape[0], dimension_input))\n    batch_y = np.zeros((Y_test.shape[0], np.unique(Y_test).shape[0]))\n        \n    for k in range(Y_test.shape[0]):\n        emb_data = np.zeros((dimension, dimension_input), dtype = np.float32)\n        for _, text in enumerate(X_test[k].split()):\n            # if the word got in the vocab\n            try:\n                emb_data[:, vocab.index(text)] += vectors[dictionary[text], :]\n            # if not, skip\n            except:\n                continue\n                \n        batch_y[k, int(Y_test[k])] = 1.0 \n        batch_x[:, k, :] = emb_data[:, :]\n            \n    testing_acc, logits = sess.run([model.accuracy, tf.cast(tf.argmax(model.logits, 1), tf.int32)], feed_dict = {model.X : batch_x, model.Y : batch_y})\n    print ('testing accuracy: ', testing_acc)\n    ACC_TEST.append(testing_acc)\n    print (metrics.classification_report(Y_test, logits, target_names = ['negative', 'positive']))\n            \nplt.subplot(1, 2, 1)\nx_component = [i for i in range(len(LOST))]\nplt.plot(x_component, LOST)\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.subplot(1, 2, 2)\nplt.plot(x_component, ACC_TRAIN, label = 'train accuracy')\nplt.plot(x_component, ACC_TEST, label = 'test accuracy')\nplt.legend()\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.show()"},{"cell_type":"markdown","metadata":{"_uuid":"1251244d9a7f7336ce1ce0220d8a702eec566898","_cell_guid":"6b76ccd9-bcb8-4982-a610-b85d0e21d1f7"},"source":"Done! Validation set is pretty fine, achieved around 90%, but after 5th epoch, it slided down a bit because the model fitted with training set."},{"cell_type":"markdown","metadata":{"_uuid":"3573487ff96b3ec8117b6f6827b394a448c8a798","_cell_guid":"f0f8ab74-dad8-4113-bfcc-9392e9686b36"},"source":"I will not do the testing dataset because it was not labelled, if you want to do it, fork this notebook"}],"nbformat_minor":1}